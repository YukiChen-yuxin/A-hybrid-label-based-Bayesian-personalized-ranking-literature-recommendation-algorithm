{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所有代码按顺序直接运行即可 此文件中部分代码未显示出结果请看 测试_含部分运行结果代码 文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore',message = \"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#已进行过处理，筛选出所有个人用户\n",
    "download=pd.read_csv(\"E:\\\\2020 慧源共享·数据悦读第二届高校开放数据创新研究大赛\\\\数据1\\\\下载_个人.csv\",error_bad_lines=False)\n",
    "see=pd.read_csv(\"E:\\\\2020 慧源共享·数据悦读第二届高校开放数据创新研究大赛\\\\数据1\\\\浏览_个人.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#已进行过处理，导入总分类数据\n",
    "f=open('总分类.txt','r')\n",
    "a=f.read()\n",
    "classfy=eval(a)\n",
    "f.close()\n",
    "\n",
    "article_classfy=copy.deepcopy(classfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入数据清理后的分类数据\n",
    "f=open('处理好的文章分类.txt','r')\n",
    "a=f.read()\n",
    "article_classfy=eval(a)\n",
    "f.close()\n",
    "f=open('处理好的文章对应字典.txt','r')\n",
    "a=f.read()\n",
    "article_dict=eval(a)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入用户字典数据\n",
    "f=open('用户_论文.txt','r')\n",
    "a=f.read()\n",
    "user_ratings=eval(a)\n",
    "f.close()\n",
    "f=open('用户_论文_次数.txt','r')\n",
    "a=f.read()\n",
    "user_ratings_count=eval(a)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入处理后的用户字典数据\n",
    "f=open('用户_论文_del.txt','r')\n",
    "a=f.read()\n",
    "user_ratings_del=eval(a)\n",
    "f.close()\n",
    "f=open('用户_论文_次数_del.txt','r')\n",
    "a=f.read()\n",
    "user_ratings_count_del=eval(a)\n",
    "f.close()\n",
    "\n",
    "user_ratings_del2=copy.deepcopy(user_ratings_del)\n",
    "user_ratings_count_del2=copy.deepcopy(user_ratings_count_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入处理后的按中图分类号给用户分类的字典\n",
    "f=open('用户_分类号_次数.txt','r')\n",
    "a=f.read()\n",
    "user_classcode=eval(a)\n",
    "f.close()\n",
    "f=open('分类号_用户.txt','r')\n",
    "a=f.read()\n",
    "user_classfy=eval(a)\n",
    "f.close()\n",
    "\n",
    "user_have=list(user_ratings_del2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入id字符串转变成序号的字典\n",
    "f=open('用户_论文_序号.txt','r')\n",
    "a=f.read()\n",
    "user_ratings_del2_num=eval(a)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 已提前处理过并存起来的数据的处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#首先定大范围分类来测试推荐准确性，按中图分类号第一个字母进行分类\n",
    "#首先看此文章对应的中图分类号的长度，大于一的话可能需要归为多类\n",
    "classfy={}\n",
    "for aid in article_dict.keys():\n",
    "    num=len(article_dict[aid])#长度\n",
    "    if num==1:\n",
    "        code=article_dict[aid][0][0]\n",
    "        if code in classfy:\n",
    "            if aid not in classfy[code]:\n",
    "                classfy[code].append(aid)\n",
    "        else:\n",
    "            classfy[code]=[]\n",
    "            classfy[code].append(aid)\n",
    "    else:\n",
    "        for i in range(0,num):\n",
    "            code=article_dict[aid][i][0]\n",
    "            if code in classfy:\n",
    "                if aid not in classfy[code]:\n",
    "                    classfy[code].append(aid)\n",
    "            else:\n",
    "                classfy[code]=[]\n",
    "                classfy[code].append(aid)       '''\n",
    "\n",
    "'''#存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('总分类.txt','w')\n",
    "f.write(str(classfy))\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#把类别错误的文献删去\n",
    "\n",
    "#首先把分类号开头是一个空格的文献的空格删去\n",
    "a_id=article_classfy[' ']\n",
    "index=article_list.index(a_id[0])\n",
    "article_dict[a_id[0]][1]=article_dict[a_id[0]][1][1:]\n",
    "article_classfy[article_dict[a_id[0]][1][0]].append(a_id[0])\n",
    "del article_classfy[' ']\n",
    "\n",
    "classcode[index]=article_dict[a_id[0]][0]+';'+article_dict[a_id[0]][1]\n",
    "\n",
    "\n",
    "#把分类号开头是中文的前面几个中文删去\n",
    "a_id=article_classfy['中']\n",
    "index=article_list.index(a_id[0])\n",
    "article_dict[a_id[0]][0]=article_dict[a_id[0]][0][7:]\n",
    "article_classfy[article_dict[a_id[0]][0][0]].append(a_id[0])\n",
    "del article_classfy['中']\n",
    "\n",
    "classcode[index]=article_dict[a_id[0]][0]\n",
    "\n",
    "\n",
    "#把分类号开头数字或其他符号的论文删去\n",
    "nega_code=[]\n",
    "for code in article_classfy.keys():\n",
    "    if code.isalpha():\n",
    "        #当确定为字母时。全部转化为大写字母\n",
    "        if code.islower():\n",
    "            code_to_upper=code.upper()\n",
    "            ar=article_classfy[code]\n",
    "            nega_code.append(code)\n",
    "            for a_c in ar:\n",
    "                article_classfy[code_to_upper].append(a_c)\n",
    "                \n",
    "            #在article_dict中找到相应的图文分类号，转化成大写字母\n",
    "            a_id=article_classfy[code]\n",
    "            for i in a_id:\n",
    "                i_code=article_dict[i]\n",
    "                for i_c in range(0,len(i_code)):\n",
    "                    if i_code[i_c][0].islower():\n",
    "                        article_dict[i][i_c]=article_dict[i][i_c][0].upper()+article_dict[i][i_c][1:]\n",
    "                        \n",
    "    else:\n",
    "        #当不是字母时,从article_classfy article_dict中删除\n",
    "        #提取出所有不符合条件的论文名\n",
    "        a_id=article_classfy[code]\n",
    "        #把这些论文中不符合条件的图文分类号从dict中删除\n",
    "        for a in a_id:\n",
    "            #检查dict中是否还有别的分类号\n",
    "            co=article_dict[a]\n",
    "            if len(co)>1:\n",
    "                for c in co:\n",
    "                    if c[0]==code:\n",
    "                        article_dict[a].remove(c)\n",
    "            else:\n",
    "                del article_dict[a]\n",
    "        #删除classfy中不符合条件的分类号项\n",
    "        nega_code.append(code)\n",
    "        \n",
    "for nega in nega_code:\n",
    "    del article_classfy[nega]\n",
    "    \n",
    "#存储处理好的文章分类\n",
    "#生成的字典存成txt的方法\n",
    "f=open('处理好的文章分类.txt','w')\n",
    "f.write(str(article_classfy))\n",
    "f.close()\n",
    "f=open('处理好的文章对应字典.txt','w')\n",
    "f.write(str(article_dict))\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#建立用户：互动文章字典\n",
    "user_ratings={}  #普通的用户：互动文章 字典\n",
    "user_ratings_count={}    #用户：[互动文章][对应次数] 字典\n",
    "I=range(temp_all_u_a.index.size)\n",
    "for i in I:\n",
    "    user_id=temp_all_u_a.loc[i,'user_id']\n",
    "    article_id=temp_all_u_a.loc[i,'article_id']\n",
    "    count=temp_all_u_a.loc[i,'count']\n",
    "    \n",
    "    if user_id in user_ratings.keys():\n",
    "        if article_id in user_ratings[user_id]:\n",
    "            continue\n",
    "        else:\n",
    "            user_ratings[user_id].append(article_id)\n",
    "            user_ratings_count[user_id]['article'].append(article_id)\n",
    "            user_ratings_count[user_id]['count'].append(count)\n",
    "    else:\n",
    "        user_ratings[user_id]=[]\n",
    "        user_ratings_count[user_id]={}\n",
    "        user_ratings_count[user_id]['article']=[]\n",
    "        user_ratings_count[user_id]['count']=[]\n",
    "        user_ratings[user_id].append(article_id)\n",
    "        user_ratings_count[user_id]['article'].append(article_id)\n",
    "        user_ratings_count[user_id]['count'].append(count)\n",
    "        '''\n",
    "\n",
    "'''#字典存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('用户_论文.txt','w')\n",
    "f.write(str(user_ratings))\n",
    "f.close()\n",
    "f=open('用户_论文_次数.txt','w')\n",
    "f.write(str(user_ratings_count))\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#提取所有有中图分类号的论文\n",
    "article_have=list(article_dict.keys())\n",
    "\n",
    "#无中图分类号的论文从用户字典中删去\n",
    "user_ratings_del={}\n",
    "user_ratings_count_del={}\n",
    "for user in user_ratings.keys():\n",
    "    index=user_ratings[user]\n",
    "    count=user_ratings_count[user]['count']\n",
    "    user_ratings_del[user]=[]\n",
    "    user_ratings_count_del[user]={}\n",
    "    user_ratings_count_del[user]['article']=[]\n",
    "    user_ratings_count_del[user]['count']=[]\n",
    "    for i in range(0,len(index)):\n",
    "        if index[i] in article_have:\n",
    "            user_ratings_del[user].append(index[i])\n",
    "            user_ratings_count_del[user]['article'].append(index[i])\n",
    "            user_ratings_count_del[user]['count'].append(count[i])\n",
    "            \n",
    "            \n",
    "#字典存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('用户_论文_del.txt','w')\n",
    "f.write(str(user_ratings_del))\n",
    "f.close()\n",
    "f=open('用户_论文_次数_del.txt','w')\n",
    "f.write(str(user_ratings_count_del))\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#按中图分类号的给用户分类\n",
    "user_classfy={}\n",
    "user_classcode={}\n",
    "for user in user_ratings_del2.keys():\n",
    "    article=user_ratings_del2[user]\n",
    "    count=user_ratings_count_del2[user]['count']\n",
    "    code=[]\n",
    "    for idd in article:\n",
    "        a_code=article_dict[idd]\n",
    "        t=[]\n",
    "        for c in a_code:\n",
    "            t.append(c[0])\n",
    "        code.append(t)\n",
    "    \n",
    "    #记录偏爱的classcode类的比例\n",
    "    user_classcode[user]={}\n",
    "    for cc in code:\n",
    "        if type(cc)==list:\n",
    "            for c in cc:\n",
    "                if c not in user_classcode[user].keys():\n",
    "                    user_classcode[user][c]=0\n",
    "                    user_classcode[user][c]=count[code.index(cc)]\n",
    "                else:\n",
    "                    user_classcode[user][c]+=count[code.index(cc)]\n",
    "        else:\n",
    "            if cc not in user_classcode[user].keys():\n",
    "                user_classcode[user][cc]=0\n",
    "                user_classcode[user][cc]=count[code.index(cc)]\n",
    "            else:\n",
    "                user_classcode[user][cc]+=count[code.index(cc)]\n",
    "    c1=[]\n",
    "    for cc in code:\n",
    "        if type(cc)==list:\n",
    "            for c in cc:\n",
    "                c1.append(c)\n",
    "        else:\n",
    "            c1.append(cc)\n",
    "    code=list(set(c1))\n",
    "    for co in code:\n",
    "        if co in user_classfy.keys():\n",
    "            if user not in user_classfy[co]:\n",
    "                user_classfy[co].append(user)\n",
    "        else:\n",
    "            user_classfy[co]=[]\n",
    "            user_classfy[co].append(user)\n",
    "            \n",
    "            \n",
    "#通过观察user_classfy可以得出 还是存在部分未清理完全的数据 进行单独清理\n",
    "user=user_classfy['3']\n",
    "a1=user_ratings[user[0]]\n",
    "a2=user_ratings[user[1]]\n",
    "a3=user_ratings[user[2]]\n",
    "for a in a1:\n",
    "    co=article_dict[a]\n",
    "    for c in co:\n",
    "        if c[0]=='3':\n",
    "            print(a)\n",
    "            print(c)\n",
    "print('next')\n",
    "for a in a2:\n",
    "    co=article_dict[a]\n",
    "    for c in co:\n",
    "        if c[0]=='3':\n",
    "            print(a)\n",
    "            print(c)\n",
    "print('next')\n",
    "for a in a3:\n",
    "    if a in article_dict.keys():\n",
    "        co=article_dict[a]\n",
    "        for c in co:\n",
    "            if c[0]=='3':\n",
    "                print(a)\n",
    "                print(c)\n",
    "                \n",
    "                \n",
    "article_dict['HY000001473620'][1]=article_dict['HY000001473620'][1][2:]\n",
    "article_dict['HY000001473620'][2]=article_dict['HY000001473620'][2][2:]\n",
    "article_dict['HY000001473620'][3]=article_dict['HY000001473620'][3][2:]\n",
    "\n",
    "article_dict['HY000001609650'][1]=article_dict['HY000001609650'][1][2:]\n",
    "article_dict['HY000001609650'][2]=article_dict['HY000001609650'][2][2:]\n",
    "\n",
    "article_dict['HY000001657044'][1]=article_dict['HY000001657044'][1][2:]\n",
    "article_dict['HY000001657044'][2]=article_dict['HY000001657044'][2][2:]\n",
    "\n",
    "article_dict['HY000002163199'][1]=article_dict['HY000002163199'][1][2:]\n",
    "article_dict['HY000002163199'][2]=article_dict['HY000002163199'][2][2:]\n",
    "\n",
    "article_dict['HY000002221331'][1]=article_dict['HY000002221331'][1][2:]\n",
    "article_dict['HY000002221331'][2]=article_dict['HY000002221331'][2][2:]\n",
    "\n",
    "article_dict['HY000002314115'][1]=article_dict['HY000002314115'][1][2:]\n",
    "article_dict['HY000002314115'][2]=article_dict['HY000002314115'][2][2:]\n",
    "\n",
    "\n",
    "#重新运行按中图分类号的给用户分类的那一段代码\n",
    "\n",
    "#字典存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('用户_分类号_次数.txt','w')\n",
    "f.write(str(user_classcode))\n",
    "f.close()\n",
    "f=open('分类号_用户.txt','w')\n",
    "f.write(str(user_classfy))\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#构造把各自id转变成列表中序号的字典\n",
    "#论文字典article_have\n",
    "#用户字典user_have\n",
    "user_ratings_del2_num={}\n",
    "for user in user_ratings_del2.keys():\n",
    "    index_user=user_have.index(user)\n",
    "    user_ratings_del2_num[index_user]=[]\n",
    "    arti=user_ratings_del2[user]\n",
    "    for i in range(0,len(arti)):\n",
    "        index_article=article_have.index(arti[i])\n",
    "        user_ratings_del2_num[index_user].append(index_article)\n",
    "        \n",
    "        \n",
    "#字典存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('用户_论文_序号.txt','w')\n",
    "f.write(str(user_ratings_del2_num))\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download=download.reset_index()\n",
    "see=see.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article=set(list(download['article_id'].values)+list(see['article_id'].values))\n",
    "article_id=list(article)\n",
    "article_id=article_id[1:]\n",
    "article_id.sort(reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down_article_classcode=download.drop(columns=['user_id','user_type','date_time','article_title','keywords','author','unit','type','province'])\n",
    "see_article_classcode=see.drop(columns=['user_id','user_type','date_time','article_title','keywords','author','unit','type','province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去重\n",
    "down_a_c=down_article_classcode.groupby(['article_id','classcode']).size().reset_index(name='count')\n",
    "see_a_c=see_article_classcode.groupby(['article_id','classcode']).size().reset_index(name='count')\n",
    "\n",
    "#两个数据框合到一起\n",
    "test_down=down_a_c.drop(columns=['count'])\n",
    "test_see=see_a_c.drop(columns=['count'])\n",
    "test=pd.concat([test_down,test_see])\n",
    "\n",
    "#去重\n",
    "test_set=test.groupby(['article_id','classcode']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#article_list和classcode提取出来建立列表\n",
    "article_list=list(test_set['article_id'].values)\n",
    "classcode=list(test_set['classcode'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#检查是否有中图分类号形式不正确,进行部分数据清理\n",
    "for i in range(0,len(classcode)-1):\n",
    "    if \"\\n\" in classcode[i]:\n",
    "        classcode[i]=classcode[i][:6]+classcode[i][-7:-1]\n",
    "        \n",
    "\n",
    "#制作文献名对应拆开的中图分类号的字典\n",
    "article_dict={}\n",
    "for i in range(0,len(classcode)):\n",
    "    if ';' in classcode[i]:\n",
    "        code=classcode[i].split(';')\n",
    "    else:\n",
    "        code=[classcode[i]]\n",
    "    article_dict[article_list[i]]=code\n",
    "    \n",
    "#检查是否有空字符单独成为列表中的某个元素,删除空字符元素\n",
    "for aid in article_dict.keys():\n",
    "    if '' in article_dict[aid]:\n",
    "        article_dict[aid].remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#制作每个类别对应的用户字典\n",
    "\n",
    "users=set(list(download['user_id'].values)+list(see['user_id'].values))\n",
    "users_list=list(users)\n",
    "users_list.sort(reverse=False)\n",
    "\n",
    "temp_down_u_a=download.drop(columns=['user_type','date_time','article_title','keywords','author','unit','type','province','classcode'])\n",
    "temp_see_u_a=see.drop(columns=['user_type','date_time','article_title','keywords','author','unit','type','province','classcode'])\n",
    "\n",
    "temp_down_u_a=temp_down_u_a.groupby(['user_id','article_id']).size().reset_index(name='count')\n",
    "temp_see_u_a=temp_see_u_a.groupby(['user_id','article_id']).size().reset_index(name='count')\n",
    "\n",
    "t1=temp_down_u_a.drop(columns=['count'])\n",
    "t2=temp_see_u_a.drop(columns=['count'])\n",
    "#合并\n",
    "temp_all_u_a=pd.concat([t1,t2])\n",
    "#去重\n",
    "temp_all_u_a=temp_all_u_a.groupby(['user_id','article_id']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_not_0=[]\n",
    "user_not_1=[]\n",
    "for user in user_ratings_del2.keys():\n",
    "    index=user_ratings_del2[user]\n",
    "    num=len(index)\n",
    "    if num==0:\n",
    "        user_not_0.append(user)\n",
    "    elif num==1:\n",
    "        user_not_1.append(user)\n",
    "        \n",
    "#删去只跟一个或零个有分类号的论文发生过关系的用户\n",
    "for user in user_not_0:\n",
    "    del user_ratings_del2[user]\n",
    "    del user_ratings_count_del2[user]\n",
    "for user in user_not_1:\n",
    "    del user_ratings_del2[user]\n",
    "    del user_ratings_count_del2[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入带有时间机制的模型计算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#读取含时间的数据\n",
    "\n",
    "globals = {\n",
    "    'nan': 0   \n",
    "}\n",
    "\n",
    "#存成txt的文件重新生成字典的方法\n",
    "f=open('E:\\\\2020 慧源共享·数据悦读第二届高校开放数据创新研究大赛\\\\data\\\\info_user.txt','r')\n",
    "a=f.read()\n",
    "dic=eval(a,globals)\n",
    "f.close()\n",
    "\n",
    "dic2=copy.deepcopy(dic)\n",
    "\n",
    "for user in users_list:\n",
    "    ar_list=list(dic2[user].keys())\n",
    "    for ar in ar_list:\n",
    "        if ar not in article_have:\n",
    "            del dic2[user][ar]\n",
    "            \n",
    "#去除无满足条件论文的用户\n",
    "for user in users_list:\n",
    "    if len(dic2[user])==0:\n",
    "        del dic2[user]\n",
    "        \n",
    "#去除只互动一次的用户，保证计算出的用户与user_ratings_del2中的用户一致\n",
    "u_t=[]\n",
    "for user in dic2.keys():\n",
    "    if user not in user_ratings_del2:\n",
    "        u_t.append(user)\n",
    "        \n",
    "for u in u_t:\n",
    "    del dic2[u]\n",
    "    \n",
    "#字典存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('info_dic2.txt','w')\n",
    "f.write(str(dic2))\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globals = {\n",
    "    'nan': 0   \n",
    "}\n",
    "f=open('info_dic2.txt','r')\n",
    "a=f.read()\n",
    "dic2=eval(a,globals)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进行相应算法处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def date_subtraction(date_lis,time_str2):\n",
    "    res = []\n",
    "    length = len(date_lis)\n",
    "    for i in range(length):\n",
    "        date_str1 = date_lis[i][:10]\n",
    "        date_str2 = time_str2[:10]\n",
    "        date1 = datetime.datetime.strptime(date_str1,'%Y-%m-%d').date()\n",
    "        date2 = datetime.datetime.strptime(date_str2,'%Y-%m-%d').date()\n",
    "        subtraction = (date2 - date1)\n",
    "        \n",
    "        if subtraction.days < 0 :\n",
    "            print('相差时间为负值，已取得觉绝对值')\n",
    "            res.append(-subtraction.days)\n",
    "        else:\n",
    "            res.append(subtraction.days)\n",
    "\n",
    "    return(res)\n",
    "\n",
    "max_num = 3\n",
    "make_inf = 300000\n",
    "import time\n",
    "today = time.strftime(\"%Y-%m-%d\",time.localtime(time.time()))########################日期，设置自动更新了\n",
    "def OP_essay(user_id,essay_id,max_num = 3,make_inf = 300000):\n",
    "    #reward_down = min(dic[user_id][essay_id]['下载_看_次数'][0],max_num)\n",
    "    #reward_see = min(dic[user_id][essay_id]['下载_看_次数'][1],max_num)\n",
    "    reward_down = dic2[user_id][essay_id]['下载_看_次数'][0]\n",
    "    reward_see = dic2[user_id][essay_id]['下载_看_次数'][1]\n",
    "    # 浏览是否转换成下载\n",
    "    reward_turn = 0\n",
    "    if dic2[user_id][essay_id]['下载_看_次数'][0] != 0 and dic2[user_id][essay_id]['下载_看_次数'][1] !=0:\n",
    "        reward_turn = 1\n",
    "    # 下载：时间衰减机制\n",
    "    temp = dic2[user_id][essay_id]['下载_看_时间'][0]\n",
    "    down_subtraction = [make_inf]\n",
    "    if temp:\n",
    "        down_subtraction = date_subtraction(dic2[user_id][essay_id]['下载_看_时间'][0],today)\n",
    "        \n",
    "    # 浏览：时间衰减机制\n",
    "    temp = dic2[user_id][essay_id]['下载_看_时间'][1]\n",
    "    see_subtraction = [make_inf]\n",
    "    if temp:\n",
    "        see_subtraction = date_subtraction(dic2[user_id][essay_id]['下载_看_时间'][1],today)\n",
    "        \n",
    "    return([reward_down,reward_see,reward_turn,down_subtraction,see_subtraction])\n",
    "\n",
    "def OP_user(user_id,max_num):\n",
    "    data = []\n",
    "    for user_id in dic2.keys():\n",
    "        for essay_id in dic2[user_id].keys():\n",
    "            data.append(OP_essay(user_id,essay_id,max_num = 3))\n",
    "    return(data)\n",
    "\n",
    "\n",
    "data = {'user_id':[],'essay_id':[],'reward_down':[],'reward_see':[],'reward_turn':[],'down_subtraction':[],'see_subtraction':[]}\n",
    "\n",
    "for user_id in list(dic2.keys()):\n",
    "    for essay_id in list(dic2[user_id].keys()):\n",
    "        res = OP_essay(user_id,essay_id,max_num = 3,make_inf = 300000)\n",
    "        data['user_id'].append(user_id)\n",
    "        data['essay_id'].append(essay_id)\n",
    "        data['reward_down'].append(res[0])\n",
    "        data['reward_see'].append(res[1])\n",
    "        data['reward_turn'].append(res[2])\n",
    "        data['down_subtraction'].append(res[3])\n",
    "        data['see_subtraction'].append(res[4])\n",
    " #reward_day = math.exp(-w_time*down_subtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(data)\n",
    "\n",
    "order = ['user_id', 'essay_id', 'reward_down', 'reward_see', 'reward_turn', 'down_subtraction','see_subtraction']\n",
    "data = data[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 综合评价方法 独立性权系数法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 时间机制的冷却因子\n",
    "Cooling_factor = 0.6\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def autoNorm(data):\n",
    "    \"\"\"\n",
    "    :param data: 列表\n",
    "    :return: 归一化列表\n",
    "    \"\"\"\n",
    "    arr = np.asarray(data)\n",
    "    norm_list=[]\n",
    "    for x in arr:\n",
    "        x = round(float(x - np.min(arr)) / ((np.max(arr) - np.min(arr))+0.001),4)\n",
    "        norm_list.append(x)\n",
    "\n",
    "    return norm_list\n",
    "\n",
    "\n",
    "\n",
    "# 自定义max-min 归一化函数\n",
    "def min_max_scaler(data):\n",
    "    \"\"\"\n",
    "    :param data: 输入各评价指标的数据框\n",
    "    :return: 归一化之后的数据框\n",
    "    \"\"\"\n",
    "    X = np.array(data)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_minMax = min_max_scaler.fit_transform(X)\n",
    "    X_minMax=pd.DataFrame(X_minMax,columns=data.columns.values)\n",
    "    return X_minMax\n",
    "\n",
    "\n",
    "\n",
    "def multipl(a, b):\n",
    "    \"\"\"\n",
    "    :param a: 列表1\n",
    "    :param b: 列表2\n",
    "    :return: 相乘求和之后的值\n",
    "    \"\"\"\n",
    "    sumofab = 0.0\n",
    "    for i in range(len(a)):\n",
    "        temp = a[i] * b[i]\n",
    "        sumofab += temp\n",
    "    return sumofab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    \"\"\"\n",
    "    :param x: 列表1\n",
    "    :param y: 列表2\n",
    "    :return: 复相关系数R\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    # 求和\n",
    "    sum1 = sum(x)\n",
    "    sum2 = sum(y)\n",
    "    # 求乘积之和\n",
    "    sumofxy = multipl(x, y)\n",
    "    # 求平方和\n",
    "    sumofx2 = sum([pow(i, 2) for i in x])\n",
    "    sumofy2 = sum([pow(j, 2) for j in y])\n",
    "    num = sumofxy - (float(sum1) * float(sum2) / n)\n",
    "    # 计算复相关系数\n",
    "    den = math.sqrt((sumofx2 - float(sum1 ** 2) / n) * (sumofy2 - float(sum2 ** 2) / n))\n",
    "    R=num / den\n",
    "    return R\n",
    "\n",
    "\n",
    "def get_R_coefficient(data):\n",
    "    \"\"\"\n",
    "    :param data: 评价指标的数据框\n",
    "    :return:评价指标的复相关系数\n",
    "    \"\"\"\n",
    "    R_list=[]\n",
    "    for i in range(0,len(data.columns.values)):\n",
    "        columns_list = list(data.columns.values)\n",
    "        k1 = columns_list[i]\n",
    "        columns_list.remove(k1)\n",
    "        X = sm.add_constant(data.loc[:,columns_list])\n",
    "        y = data.loc[:,k1]\n",
    "        est = sm.OLS(y.astype(float), X.astype(float))\n",
    "        est2 = est.fit()\n",
    "        predict = est2.predict()\n",
    "        R=corrcoef(y, predict)\n",
    "        R_list.append(R)\n",
    "\n",
    "    return R_list\n",
    "\n",
    "\n",
    "def get_R_wi(R_list):\n",
    "    \"\"\"\n",
    "    :param R_list: 复相关系数列表\n",
    "    :return: 各指标的权重\n",
    "    \"\"\"\n",
    "    wi_1=[]\n",
    "    wi_list=[]\n",
    "    for i in range(0,len(R_list)):\n",
    "        # 求各指标与其他指标复相关系数的倒数\n",
    "        w1=round(1/R_list[i],2)\n",
    "        wi_1.append(w1)\n",
    "\n",
    "    # 对上表数据进行归一化处理，即可得到各指标的权重\n",
    "    sum_wi_1=sum(wi_1)\n",
    "    for i in range(0,len(wi_1)):\n",
    "        # w2 = wi_1[i] / sum_wi_1\n",
    "        w2=round(wi_1[i]/sum_wi_1,2)\n",
    "        wi_list.append(w2)\n",
    "\n",
    "\n",
    "    return wi_list\n",
    "\n",
    "one_year = 365\n",
    "Cooling_factor = 0.6\n",
    "def operate_date(train_data,Cooling_factor):\n",
    "    length = len(train_data)\n",
    "    for i in range(length):\n",
    "\n",
    "         # 下载时间机制处理\n",
    "        length2 = len(data_test['down_subtraction'][i])\n",
    "        down_subtraction = []\n",
    "\n",
    "        for j in range(length2):\n",
    "            down_subtraction.append(math.exp(-Cooling_factor*data_test['down_subtraction'][i][j]/one_year))\n",
    "        train_data[i,0] = sum(down_subtraction)\n",
    "        \n",
    "        \n",
    "        #浏览事件机制处理\n",
    "        length3 = len(data_test['see_subtraction'][i])\n",
    "        \n",
    "        \n",
    "        see_subtraction = []\n",
    "        \n",
    "        for p in range(length3):\n",
    "            see_subtraction.append(math.exp(-Cooling_factor*data_test['see_subtraction'][i][p]/one_year))\n",
    "        train_data[i,1] = sum(see_subtraction)\n",
    "      \n",
    "    return(train_data)\n",
    "\n",
    "\n",
    "def get_article_score(wi_list,data):\n",
    "    \"\"\"\n",
    "    :param wi_list: 权重系数列表\n",
    "    :param data：评价指标数据框\n",
    "    :return:返回文章质量得分\n",
    "    \"\"\"\n",
    "\n",
    "    #  将权重转换为矩阵\n",
    "\n",
    "    cof_var = np.mat(wi_list)\n",
    "\n",
    "    #  将数据框转换为矩阵\n",
    "    context_train_data = np.mat(data)\n",
    "    \n",
    "    #  权重跟自变量相乘  \n",
    "    # 自变量：context_train_data\n",
    "    # 权重：cof_var\n",
    "    context_train_data = operate_date(context_train_data,Cooling_factor=0.6)\n",
    "    \n",
    "    last_hot_matrix = context_train_data * cof_var.T\n",
    "    \n",
    "    \n",
    "    last_hot_matrix = pd.DataFrame(last_hot_matrix.T)\n",
    "    \n",
    "    \n",
    "    #  累加求和得到总分\n",
    "    last_hot_score = list(last_hot_matrix.apply(sum))\n",
    "\n",
    "    #  max-min 归一化\n",
    "\n",
    "    last_hot_score_autoNorm = autoNorm(last_hot_score)\n",
    "\n",
    "    # 值映射成分数（0-100分）？？？？？\n",
    "\n",
    "    last_hot_score_result = [i * 1000 for i in last_hot_score_autoNorm]\n",
    "\n",
    "    return last_hot_score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ##改成小数据\n",
    "    context_train_data = data.iloc[:,2:5]\n",
    "    R_list = get_R_coefficient(context_train_data)\n",
    "    wi_list = get_R_wi(R_list)\n",
    "    wi_list[2]\n",
    "    \n",
    "    last_hot_score_result = get_article_score(wi_list,context_train_data)\n",
    "\n",
    "    # 增加一列id\n",
    "    context_train_data['user_id'] = data['user_id']\n",
    "    context_train_data['essay_id'] = data['essay_id']\n",
    "    \n",
    "\n",
    "    # 增加一列分数值\n",
    "    context_train_data['score'] = last_hot_score_result\n",
    "\n",
    "    # 然后对数据框从大到小排序\n",
    "\n",
    "    #result=context_train_data.sort_values(by = 'score',axis = 0,ascending = False)\n",
    "\n",
    "    #result['rank']=range(1,len(result)+1)\n",
    "\n",
    "    #print(context_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test(user_ratings):\n",
    "    \"\"\"\n",
    "    对每一个用户u，在user_ratings中随机找到他评分过的一部电影i,保存在user_ratings_test，我们为每个用户取出的这一个电影，是不会在训练集中训练到的，作为测试集用。\n",
    "    \"\"\"\n",
    "    user_test = dict()\n",
    "    for u,i_list in user_ratings.items():\n",
    "        user_test[u] = random.sample(user_ratings[u],1)[0]\n",
    "    return user_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_batch(user_ratings,user_ratings_test,item_count):\n",
    "    \"\"\"\n",
    "    对于每个用户u，它的评分电影i是我们在user_ratings_test中随机抽取的，它的j是用户u部分没有评分过的论文集合，\n",
    "    比如用户有n个评分过的论文，那么就从互动过的分类号中随机抽取n个未评分过的论文作为j\n",
    "    \"\"\"\n",
    "    for u in user_ratings.keys():\n",
    "        #查看所有互动过的分类号\n",
    "        code=list(user_classcode[user_have[u]].keys())\n",
    "        #从这些分类号中提取出所有论文\n",
    "        ar=[]\n",
    "        for co in code:\n",
    "            ar=ar+article_classfy[co]\n",
    "            \n",
    "\n",
    "        num=len(user_ratings[u])\n",
    "        t = []\n",
    "        i = user_ratings_test[u]\n",
    "        for count in range(num):\n",
    "            j_id = random.randint(1,len(ar)-1)\n",
    "            j=article_have.index(ar[j_id])\n",
    "            if not(j in user_ratings[u]):\n",
    "                t.append([u,i,j])\n",
    "            else:\n",
    "                count-=1\n",
    "        yield np.asarray(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train_batch(user_ratings,user_ratings_test,item_count,batch_size=512):\n",
    "    \"\"\"\n",
    "    构造训练用的三元组\n",
    "    三元组的组成 (u,i,未交互论文j)或(u,分数较高的i,分数较低的i)\n",
    "    对于随机抽出的用户u，i可以从user_ratings随机抽出，而j也是从总的论文集中随机抽出\n",
    "    当然j必须保证(u,j)不在user_ratings中\n",
    "    \"\"\"\n",
    "    t = []\n",
    "    for b in range(batch_size):\n",
    "        u = random.sample(user_ratings.keys(),1)[0]\n",
    "        \n",
    "        #从dic2中提取出所有论文的排名\n",
    "        rank_index=context_train_data[(context_train_data['user_id'].values==user_have[u])].index.tolist()\n",
    "        rank_ar=list(context_train_data.loc[rank_index,'essay_id'])\n",
    "        rank_score=list(context_train_data.loc[rank_index,'score'])\n",
    "        \n",
    "        i = random.sample(user_ratings[u],1)[0]\n",
    "        #test数据集里的数据不能作为训练使用，所以要另外取新值\n",
    "        while i==user_ratings_test[u]:\n",
    "            i = random.sample(user_ratings[u],1)[0]\n",
    "\n",
    "        j = random.randint(1,item_count)\n",
    "        while j in user_ratings[u]:\n",
    "            loc1=rank_ar.index(article_have[j])\n",
    "            loc2=rank_ar.index(article_have[i])\n",
    "            if rank_score[loc1]<rank_score[loc2]:\n",
    "                break\n",
    "            else:\n",
    "                j = random.randint(1,item_count)\n",
    "\n",
    "        t.append([u,i,j])\n",
    "\n",
    "    return np.asarray(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造算法模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bpr_mf(user_count,item_count,hidden_dim):\n",
    "    #hidden_dim是我们矩阵分解的隐含维度k\n",
    "    \n",
    "    #预先给三个参数赋予占位，分配必要的内存，等运行模型的时候通过feed_dict喂入数据\n",
    "    u = tf.placeholder(tf.int32,[None])\n",
    "    i = tf.placeholder(tf.int32,[None])\n",
    "    j = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "    #创建新的tensorflow遍历，shape为count+1到hiddendim\n",
    "    #创建的是 正态分布初始化器 均值0 方差0.1\n",
    "    user_emb_w = tf.get_variable(\"user_emb_w\", [user_count + 1, hidden_dim],\n",
    "                                 initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    item_emb_w = tf.get_variable(\"item_emb_w\", [item_count + 1, hidden_dim],\n",
    "                                 initializer=tf.random_normal_initializer(0, 0.1))\n",
    "\n",
    "    #选取一个张量里面索引对应的元素\n",
    "    #u,i,j是索引,user_emb_w等是张量\n",
    "    u_emb = tf.nn.embedding_lookup(user_emb_w, u)\n",
    "    i_emb = tf.nn.embedding_lookup(item_emb_w, i)\n",
    "    j_emb = tf.nn.embedding_lookup(item_emb_w, j)\n",
    "\n",
    "    #MF predict:u_i>u_j\n",
    "    #multiply 相同位置的元素相乘\n",
    "    #reduce_sum 计算张量沿着某一维度的和\n",
    "    #keepdims是否保持原有张量的维度\n",
    "    x = tf.reduce_sum(tf.multiply(u_emb,(i_emb-j_emb)),1,keep_dims=True)\n",
    "\n",
    "    \n",
    "    #每个用户的AUC（AUC是指ROC曲线下的面积，TP越大，ROC曲线越上凸，算法分类效果越优秀）\n",
    "    #average AUC = mean( auc for each user in test set)\n",
    "    mf_auc = tf.reduce_mean(tf.to_float(x>0))\n",
    "\n",
    "    #add_n 列表的元素相加\n",
    "    l2_norm = tf.add_n([\n",
    "        tf.reduce_sum(tf.multiply(u_emb, u_emb)),\n",
    "        tf.reduce_sum(tf.multiply(i_emb, i_emb)),\n",
    "        tf.reduce_sum(tf.multiply(j_emb, j_emb))\n",
    "    ])\n",
    "\n",
    "    #损失函数\n",
    "    regulation_rate = 0.0001\n",
    "    bprloss = regulation_rate * l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n",
    "\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(bprloss)\n",
    "    return u, i, j, mf_auc, bprloss, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_test = generate_test(user_ratings_del2_num)\n",
    "\n",
    "users_temp=[]\n",
    "for u in user_ratings_del2_num:\n",
    "    users_temp.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#整理代码中暂不运行此代码，若看结果请看测试代码此部分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    u,i,j,mf_auc,bprloss,train_op = bpr_mf(len(users_temp),len(article_have),20)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(1,5):\n",
    "        _batch_bprloss = 0\n",
    "        for k in range(1,10):# uniform samples from training set5000\n",
    "            uij = generate_train_batch(user_ratings_del2_num,user_ratings_test,len(article_have))\n",
    "            \n",
    "            _bprloss,_train_op = sess.run([bprloss,train_op],\n",
    "                                          feed_dict={u:uij[:,0],i:uij[:,1],j:uij[:,2]})\n",
    "            \n",
    "            _batch_bprloss += _bprloss\n",
    "\n",
    "        print(\"epoch:\",epoch)\n",
    "        print(\"bpr_loss:\",_batch_bprloss / k)\n",
    "        print(\"_train_op\")\n",
    "\n",
    "        user_count = 0\n",
    "        _auc_sum = 0.0\n",
    "\n",
    "        # each batch will return only one user's auc\n",
    "        for t_uij in generate_test_batch(user_ratings_del2_num, user_ratings_test, len(article_have)):\n",
    "            _auc, _test_bprloss = sess.run([mf_auc, bprloss],\n",
    "                                              feed_dict={u: t_uij[:, 0], i: t_uij[:, 1], j: t_uij[:, 2]}\n",
    "                                              )\n",
    "\n",
    "            user_count += 1\n",
    "            _auc_sum += _auc\n",
    "        print(\"test_loss: \", _test_bprloss, \"test_auc: \", _auc_sum / user_count)\n",
    "        print(\"\")\n",
    "    variable_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variable_names)\n",
    "    for k, v in zip(variable_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "        print(v)\n",
    "\n",
    "#这里取了k=20，迭代次数4\n",
    "#输出的W,H矩阵分别在values[0]和values[1]中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用计算出的矩阵计算用户的推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#示例 测试用户为 '02412b4ddc1148d2'\n",
    "user_id='02412b4ddc1148d2'\n",
    "#查询用户名所对应索引\n",
    "index=[user_have.index(user_id)]\n",
    "\n",
    "#查询这个用户推荐排列前五的文献\n",
    "\n",
    "#用户对这个用户对所有电影的预测评分\n",
    "session1 = tf.Session()\n",
    "#expand_dims扩大指定维数 原来values[0][0]为（20，）的数组 变为(1,20)\n",
    "u1_dim = tf.expand_dims(values[0][index[0]], 0)\n",
    "\n",
    "#matmul a、b矩阵相乘 transpose_b：如果 True,b 在乘法之前转置\n",
    "u1_all = tf.matmul(u1_dim, values[1],transpose_b=True)\n",
    "\n",
    "#计算用户对所有商品的排序\n",
    "result_1 = session1.run(u1_all)\n",
    "\n",
    "#squeeze 将结果矩阵的第一维去掉，去掉外面一个框\n",
    "p = np.squeeze(result_1)\n",
    "\n",
    "#argsort 返回数组值从小到大的索引值\n",
    "#把大小除了最大五个以外的p里的其他索引对应的值都设为0\n",
    "p[np.argsort(p)[:-5000]] = 0\n",
    "\n",
    "#遍历p中的全部索引，只要值不为0，就输出索引和对应的索引值\n",
    "article_rec=[]\n",
    "score=[]\n",
    "for index in range(len(p)):\n",
    "    if p[index] != 0:\n",
    "        score.append([index,p[index]])\n",
    "        article_rec.append(article_have[index])\n",
    "        \n",
    "#转为数据框\n",
    "score_df=pd.DataFrame(score)\n",
    "\n",
    "#按顺序进行排序\n",
    "score_df2=score_df.sort_values(by=1, ascending=False)\n",
    "\n",
    "#筛选出这个用户感兴趣的分类号，四舍五入按比例选出前200个论文\n",
    "co=list(user_classcode['02412b4ddc1148d2'].keys())\n",
    "co_sum=np.sum(list(user_classcode['02412b4ddc1148d2'].values()))\n",
    "co_wi=[]\n",
    "for c in co:\n",
    "    co_wi.append(int(round(user_classcode['02412b4ddc1148d2'][c]/co_sum)))\n",
    "ar_incode=[]\n",
    "for c in co:\n",
    "    flag=0\n",
    "    for ar_index in list(score_df2[0].values):\n",
    "        if article_have[ar_index] in article_classfy[c]:\n",
    "            ar_incode.append(article_have[ar_index])\n",
    "            flag+=1\n",
    "            if flag>=200:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在测试过程中推荐的200个文献列表\n",
    "f=open('对于某用户的推荐前200位.txt','r')\n",
    "a=f.read()\n",
    "ar_incode=eval(a)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ddyylc201801180',\n",
       " 'tjzyxyxb201601007',\n",
       " 'gwyx-lcsw201916022',\n",
       " 'zdxllysj201902003',\n",
       " 'xtyx201710015',\n",
       " 'zgetbjzz200201024',\n",
       " 'Y2257638',\n",
       " 'zgyyzn201708202',\n",
       " 'zfysjjbzz201603010',\n",
       " 'sczlfz200602012',\n",
       " 'xzy200611021',\n",
       " 'ylsbxx201907020',\n",
       " 'xngfyy201901018',\n",
       " 'ddhs-xsb201901048',\n",
       " 'syxnfxgbzz201903009',\n",
       " 'jslcyxzz201613048',\n",
       " 'zhqkyszz200704022',\n",
       " 'jtbj201923286',\n",
       " 'D01297221',\n",
       " 'Y2248365',\n",
       " 'gdyx201605030',\n",
       " 'zgbjyy-kp201703250',\n",
       " 'zghlgl201901027',\n",
       " 'yiyqy201731086',\n",
       " 'nmgyxyxb201905005',\n",
       " '7692269',\n",
       " 'zgazbxb201704011',\n",
       " 'xbyx201310011',\n",
       " 'tjyy201601026',\n",
       " 'sxhlzz201914032',\n",
       " 'D784482',\n",
       " 'jxyy201603025',\n",
       " 'qlhlzz201011056',\n",
       " 'yxzs201901030',\n",
       " 'zgxdyxzz201908028',\n",
       " 'lcxhbzz200202021',\n",
       " 'zglcsyyx201506017',\n",
       " 'yxfwyyj201803022',\n",
       " '7407212',\n",
       " 'szybj201922019',\n",
       " 'Y3119709',\n",
       " 'zjzyxyxb200901080',\n",
       " 'hyyjk-syzl201412053',\n",
       " 'hngfyxzz201703021',\n",
       " 'njbdyy201803019',\n",
       " 'dsjydxxb201422003',\n",
       " 'cqyx201335014',\n",
       " 'zgbjyy-kp201624113',\n",
       " 'zgkjzh201513159',\n",
       " 'zgbjyy-kp201936032',\n",
       " 'zhzl201503007',\n",
       " 'shandyy200303035',\n",
       " 'gwyx-shyx201805023',\n",
       " 'xdzxyjhzz200119087',\n",
       " 'gxyx201904016',\n",
       " 'QK200001521324',\n",
       " 'zhsjwkjbyjzz200804015',\n",
       " 'yiyqy201930029',\n",
       " 'bjyx201409006',\n",
       " 'rdyxzz200506012',\n",
       " 'zgsyyy201126168',\n",
       " 'zsykdxxb201804008',\n",
       " 'zgyjkjzz200306023',\n",
       " 'zgzyyxdycjy201209070',\n",
       " 'zhfck201112023',\n",
       " 'yxyzx201808011',\n",
       " 'jkbd-z201211132',\n",
       " 'hsjxzz201601010',\n",
       " 'zylcyj201929003',\n",
       " 'zhongcy200602029',\n",
       " 'cqykdxxb201810014',\n",
       " 'ahwszyjsxyxb200702049',\n",
       " 'zgyfyxzz201611013',\n",
       " 'yinsbj201713201',\n",
       " 'zgyygl201308031',\n",
       " 'xlyk201901081',\n",
       " 'qkkqyxdzzz201933135',\n",
       " 'hnsysjjbzz201321063',\n",
       " 'D01202103',\n",
       " 'qlhlzz201503020',\n",
       " 'yxllysj201119031',\n",
       " '5614053',\n",
       " 'zgwcwkzz201905008',\n",
       " 'hhyy201503017',\n",
       " 'zyyxk200803022',\n",
       " 'zgxdxzz201603016',\n",
       " 'Y1986002',\n",
       " 'lcyydzzz201907172',\n",
       " 'Y2559551',\n",
       " 'syzlzz201902017',\n",
       " 'HYC201402250000014641',\n",
       " 'ylsbxx201908024',\n",
       " 'tjyy200702005',\n",
       " 'xbyxzz201903008',\n",
       " 'zhongcy201004004',\n",
       " 'xlys-x201827080',\n",
       " '0120181103910625',\n",
       " 'zgyxwz-jhsyfckxfc201902010',\n",
       " 'tjzyxyxb201903015',\n",
       " 'zgyxwz-lnyxfc201704031',\n",
       " 'Y2791075',\n",
       " 'yycyzx201822038',\n",
       " 'lcysyblxzz201709018',\n",
       " 'zhek201912020',\n",
       " '42680',\n",
       " 'zhcs201207022',\n",
       " 'yinsbj201919060',\n",
       " 'jtbj201923033',\n",
       " 'syazzz201907048',\n",
       " 'hbyy201823029',\n",
       " 'zhmzxzz98199907010',\n",
       " 'qlzlzz200312033',\n",
       " 'zgydyxzz200104001',\n",
       " 'zhnfmdx201812012',\n",
       " 'qkkqyxdzzz201926143',\n",
       " 'jkr201932006',\n",
       " 'gdybfz201606009',\n",
       " 'lcyydzzz201815106',\n",
       " 'zhxwwk201203020',\n",
       " 'yxyjyyxyy201921016',\n",
       " 'zywxzz201404012',\n",
       " 'Y2583223',\n",
       " 'zglcylxzz201924004',\n",
       " 'ncdxxb201506017',\n",
       " 'hsjxzz201504051',\n",
       " 'syyxzz201218032',\n",
       " 'xhyx201905019',\n",
       " 'syfckzz201307002',\n",
       " 'zgbjyy201312459',\n",
       " 'jtbj202013373',\n",
       " 'lchlyyzz201333076',\n",
       " 'xlyk201912079',\n",
       " 'xbyx201912005',\n",
       " 'zgnjzz200706023',\n",
       " 'dzlcgw200709037',\n",
       " 'yxzs200619020',\n",
       " 'zhpf201910010',\n",
       " 'D723461',\n",
       " 'D01442418',\n",
       " 'zhonghfbjbzz201501005',\n",
       " 'Y1300584',\n",
       " 'jkdsy-xsb201924109',\n",
       " 'yycyzx201516029',\n",
       " 'dajjk201708335',\n",
       " 'xdlcyxswgcxzz201901012',\n",
       " 'sjzxyy-e201529165',\n",
       " 'zgtnbzz200504011',\n",
       " 'ylzb201915042',\n",
       " 'wkllysj201102031',\n",
       " 'zhhlzz199902018',\n",
       " 'zgsyyy200916169',\n",
       " 'jkdsy-xsb201922273',\n",
       " 'zglcylxzz200906014',\n",
       " 'zgwsjj201707007',\n",
       " 'xdhl201909005',\n",
       " 'nfhlxb201111025',\n",
       " 'zgwsbzgl201824034',\n",
       " 'qlhlzz201214026',\n",
       " 'QK199800572333',\n",
       " 'zgkf201903012',\n",
       " 'cqyx201128001',\n",
       " 'ylbjqj201905059',\n",
       " 'zgxdyxzz201907012',\n",
       " 'hsjxzz201813011',\n",
       " 'zgmyxzz201710005',\n",
       " 'sxzlyx201717010',\n",
       " 'ylzb201919111',\n",
       " 'zgjscxyjscbzz201003022',\n",
       " 'zgjxyxjy201919062',\n",
       " 'zgyszz2014z2093',\n",
       " 'zhfck201812006',\n",
       " 'lnyxzz200301009',\n",
       " 'shandyy201725022',\n",
       " 'Y1309063',\n",
       " 'Y1272536',\n",
       " 'zgyyzn201410115',\n",
       " 'tjykdxxb201803013',\n",
       " 'yinsbj201947193',\n",
       " 'gxyx201901018',\n",
       " 'lcykzz201506007',\n",
       " 'hnsysjjbzz201724026',\n",
       " 'jkdsy-xsb2013050271',\n",
       " 'jkdsy-xsb201904097',\n",
       " 'zhpf201811016',\n",
       " 'QK200302129412',\n",
       " 'zgbjyy-kp201806176',\n",
       " 'yinsbj201725012',\n",
       " 'lcyydzzz201953068',\n",
       " 'zhhlzz201510019',\n",
       " 'zjyx201110010',\n",
       " 'syfknfm-e201906016',\n",
       " 'D791137',\n",
       " 'zgsyxyxzz201702006',\n",
       " 'gwyx-zlxfc201205014',\n",
       " 'hxyxzz201703034',\n",
       " 'qlhlzz201921004',\n",
       " 'lincjyzz201309036',\n",
       " 'Y845058',\n",
       " 'yydb201905021',\n",
       " 'jxyy201908002']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_incode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#制作便于查找论文相关信息的数据框\n",
    "down_article_df=download.drop(columns=['user_id','user_type','date_time','keywords','author','unit','type','province'])\n",
    "see_article_df=see.drop(columns=['user_id','user_type','date_time','keywords','author','unit','type','province'])\n",
    "#合并\n",
    "all_article_df=pd.concat([down_article_df,see_article_df])\n",
    "#去重\n",
    "all_article_df=all_article_df.groupby(['article_id','article_title','classcode']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#挑选排列前15个文献，存储她们的各信息\n",
    "recom_article_id=[]\n",
    "recom_article_classcode=[]\n",
    "recom_article_title=[]\n",
    "recom_score=[]\n",
    "for ar in ar_incode[0:15]:\n",
    "    if ar in all_article_df['article_id'].values:\n",
    "        recom_article_id.append(ar)\n",
    "        recom_article_classcode.append(all_article_df.loc[all_article_df[(all_article_df['article_id'].values==ar)].index.tolist()[0],'classcode'])\n",
    "        recom_article_title.append(all_article_df.loc[all_article_df[(all_article_df['article_id'].values==ar)].index.tolist()[0],'article_title'])\n",
    "        for i in score:\n",
    "            if article_have.index(ar)==i[0]:\n",
    "                recom_score.append(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 抽样计算覆盖率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#由于电脑以及tensorflow框架的局限性，随机取100个用户进行推荐，取每个用户推荐的前10个论文\n",
    "all_dic={}\n",
    "for i in range(0,10):\n",
    "    user_id=random.sample(user_have,1)[0]\n",
    "    #查询用户名所对应索引\n",
    "    index=[user_have.index(user_id)]\n",
    "\n",
    "    #用户对这个用户对所有电影的预测评分\n",
    "    session1 = tf.Session()\n",
    "    #expand_dims扩大指定维数 原来values[0][0]为（20，）的数组 变为(1,20)\n",
    "    u1_dim = tf.expand_dims(values[0][index[0]], 0)\n",
    "    \n",
    "    #matmul a、b矩阵相乘 transpose_b：如果 True,b 在乘法之前转置\n",
    "    u1_all = tf.matmul(u1_dim, values[1],transpose_b=True)\n",
    "\n",
    "    #计算用户对所有商品的排序\n",
    "    result_1 = session1.run(u1_all)\n",
    "    \n",
    "    #squeeze 将结果矩阵的第一维去掉，去掉外面一个框\n",
    "    p = np.squeeze(result_1)\n",
    "    \n",
    "    #argsort 返回数组值从小到大的索引值\n",
    "    #把大小除了最大五个以外的p里的其他索引对应的值都设为0\n",
    "    p[np.argsort(p)[:-5000]] = 0\n",
    "\n",
    "    #遍历p中的全部索引，只要值不为0，就输出索引和对应的索引值\n",
    "    article_rec=[]\n",
    "    score=[]\n",
    "    for index in range(len(p)):\n",
    "        if p[index] != 0:\n",
    "            score.append([index,p[index]])\n",
    "            article_rec.append(article_have[index])\n",
    "            \n",
    "            \n",
    "    #转为数据框\n",
    "    score_df=pd.DataFrame(score)\n",
    "\n",
    "    #按顺序进行排序\n",
    "    score_df2=score_df.sort_values(by=1, ascending=False)\n",
    "\n",
    "    #筛选出这个用户感兴趣的分类号，四舍五入按比例选出前200个论文\n",
    "    co=list(user_classcode[user_id].keys())\n",
    "    co_sum=np.sum(list(user_classcode[user_id].values()))\n",
    "    co_wi=[]\n",
    "    for c in co:\n",
    "        co_wi.append(int(round(user_classcode[user_id][c]/co_sum)))\n",
    "    ar_incode=[]\n",
    "    for c in co:\n",
    "        flag=0\n",
    "        for ar_index in list(score_df2[0].values):\n",
    "            if article_have[ar_index] in article_classfy[c]:\n",
    "                ar_incode.append(article_have[ar_index])\n",
    "                flag+=1\n",
    "                if flag>=200*co_wi[co.index(c)]:\n",
    "                    break\n",
    "                \n",
    "                \n",
    "    recom_article_id=[]\n",
    "    for ar in ar_incode[0:15]:\n",
    "        if ar in all_article_df['article_id'].values:\n",
    "            recom_article_id.append(ar)\n",
    "                \n",
    "    all_dic[user_id]={}\n",
    "    all_dic[user_id]['recom_article_id']=recom_article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在测试过程中推荐的200个文献列表\n",
    "f=open('随机抽取的10位用户推荐的117篇文献.txt','r')\n",
    "a=f.read()\n",
    "all_dic=eval(a)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3dec9e7a8c54251c': {'recom_article_id': ['ggyzl201807014',\n",
       "   'zwjl201721039',\n",
       "   'Y3262432',\n",
       "   'kxdb-kxjy201517166',\n",
       "   'zgfdc-xx201910006',\n",
       "   'D767640',\n",
       "   'syjlr201018104',\n",
       "   'Y707824',\n",
       "   'D01390515',\n",
       "   'whqcgydxxb201505027',\n",
       "   'dzjsjjgl201904012',\n",
       "   'kjyj201110007',\n",
       "   'kjzfxd-kxyzb201106198',\n",
       "   'Y3496684',\n",
       "   'jjwtts200401004']},\n",
       " '56668dab9978ec85': {'recom_article_id': ['Y2192158',\n",
       "   '9516403',\n",
       "   'xnmzxyxb201303013',\n",
       "   'D01019667']},\n",
       " '5dcb0a55de97c649': {'recom_article_id': ['hljkjxx201010236',\n",
       "   'shkx201705007',\n",
       "   'D01702232']},\n",
       " '71aa6d67367afed6': {'recom_article_id': ['ycsfxyxb-rwsh201901003',\n",
       "   'zgzckjs201507014',\n",
       "   'fzbl201901188',\n",
       "   'zgxlwszz200301014',\n",
       "   'fzbl201901188']},\n",
       " '8626ac1c132554c5': {'recom_article_id': ['fskxjsxyxb201904002',\n",
       "   'jtysxtgcyxx201901028',\n",
       "   'xzqykj201413121',\n",
       "   'D01058974',\n",
       "   'Y2817730',\n",
       "   'cxjs201504016',\n",
       "   'D01078530',\n",
       "   'txsj201909119',\n",
       "   'qcsyjs201714058',\n",
       "   'gckz201901015',\n",
       "   'zljxysgjxh201802009',\n",
       "   'tdbzsj201606032',\n",
       "   'D004224',\n",
       "   'sjhy200305001',\n",
       "   'jtysgcxb201904007']},\n",
       " 'a222d924931bca40': {'recom_article_id': ['kawskz200605018',\n",
       "   'zgxwyxkx201112017',\n",
       "   'tlsjyyj201504046',\n",
       "   'Y1392469',\n",
       "   'D331187',\n",
       "   'zhsjwkjbyjzz200804015',\n",
       "   'hlxzz201823021',\n",
       "   'lcgdbzz201901009',\n",
       "   'zgyxwlxzz201901007',\n",
       "   'ykyj201903015',\n",
       "   'Y520857',\n",
       "   'jyyxylc200812019',\n",
       "   'qlhlzz200808067',\n",
       "   'shanxzyxyxb201106004',\n",
       "   'Y2029055']},\n",
       " 'a3c8665e86ef3abb': {'recom_article_id': ['dtsj201902076',\n",
       "   'dnzsyjs-itrzyksb200934013',\n",
       "   'hqsgkj201327262',\n",
       "   'D01772187',\n",
       "   'Y3472006',\n",
       "   'Y1774817',\n",
       "   'spgykj201713001',\n",
       "   'jskjxx201905014',\n",
       "   'dianq201202022',\n",
       "   'Y3505446',\n",
       "   'hnsfdx-zr201602006',\n",
       "   'Y2433804',\n",
       "   'zrht201703014',\n",
       "   'Y3221843',\n",
       "   'lsltjs201907052']},\n",
       " 'b5c8b51d3c7149c4': {'recom_article_id': ['zgjm201413011',\n",
       "   'zgfyxzz201806014',\n",
       "   'dsbc-llb201912014',\n",
       "   'zndxxb-shkxb201906012',\n",
       "   'bjg201604150',\n",
       "   'D575209',\n",
       "   'zflc201905003',\n",
       "   'xxyts201709012',\n",
       "   'ctfz201507169',\n",
       "   'sdsghglgbxyxb200702027',\n",
       "   'D372234',\n",
       "   'xll201808008',\n",
       "   'qnyshh201416298',\n",
       "   'albsj201906004',\n",
       "   'hgzyjsxyxb201904031']},\n",
       " 'e00afc4ec3f24966': {'recom_article_id': ['dzkj201812009',\n",
       "   'hnlgdxxb201401004',\n",
       "   'Y700032',\n",
       "   'Y1397436',\n",
       "   'D305447',\n",
       "   'Y822653',\n",
       "   'D369402',\n",
       "   'lmzydxxb201003007',\n",
       "   '7465533',\n",
       "   'D775825',\n",
       "   'yyjs201614019',\n",
       "   'zgkjzh201617077',\n",
       "   'Y3398159',\n",
       "   'Y122039',\n",
       "   'D790074']},\n",
       " 'e9a1db52797ed7a8': {'recom_article_id': ['glxdh201905026',\n",
       "   'D01633965',\n",
       "   'csfzyj200806030',\n",
       "   'jrllysj201812015',\n",
       "   'ggzscgc201202041',\n",
       "   'CASS_7002848695',\n",
       "   'D01367803',\n",
       "   'Y761458',\n",
       "   '110276',\n",
       "   'Y1334788',\n",
       "   'scxdh200721250',\n",
       "   'D526316',\n",
       "   'jzgc201929023',\n",
       "   'Y812805',\n",
       "   'xzqykj201918043']}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9914529914529915"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算覆盖率\n",
    "all_art=[]\n",
    "for user in all_dic.keys():\n",
    "    all_art+=all_dic[user]['recom_article_id']\n",
    "\n",
    "#10个用户推荐100个文献中可视为无重复推荐100*116/117个文献\n",
    "100*len(set(all_art))/len(all_art)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 论文部分数据计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum=0\n",
    "for code in article_classfy.keys():\n",
    "    sum+=len(article_classfy[code])\n",
    "#计算论文标签权重 qi/q总\n",
    "w_code={}\n",
    "for code in article_classfy.keys():\n",
    "    w_code[code]=len(article_classfy[code])/sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算用户对某个标签的权重，即(此用户对这个标签的互动次数/此用户互动总次数)*此标签的权重\n",
    "w_user={}\n",
    "for user in user_classcode.keys():\n",
    "    c_sum=np.sum(list(user_classcode[user].values()))\n",
    "    for c in user_classcode[user]:\n",
    "        if user in w_user.keys():\n",
    "            w_user[user][c]=(user_classcode[user][c]/c_sum)*w_code[c]\n",
    "        else:\n",
    "            w_user[user]={}\n",
    "            w_user[user][c]=(user_classcode[user][c]/c_sum)*w_code[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#字典存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('用户标签权重.txt','w')\n",
    "f.write(str(w_user))\n",
    "f.close()\n",
    "\n",
    "#字典存储\n",
    "#生成的字典存成txt的方法\n",
    "f=open('论文标签权重.txt','w')\n",
    "f.write(str(w_code))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
